import os, sys
import time
import psutil
import threading
import ftplib
import pandas as pd
import numpy as np

sys.path.append("/data/faird")
from sdk.dacp_client import DacpClient, Principal

from utils.logger_utils import get_logger
logger = get_logger(__name__)

# ===== åŸºç¡€é…ç½® =====
SERVER_URL = "dacp://10.0.89.38:3101"
USERNAME = "user1@cnic.cn"
PASSWORD = "user1@cnic.cn"
TENANT = "conet"

FTP_HOST = "10.0.89.38"
FTP_USER = "ftpuser"
FTP_PASS = "ftpuser"

CONCURRENCY = 5  # å¹¶å‘çº¿ç¨‹æ•°
RUN_TIMES = 3  # æ¯é¡¹æµ‹è¯•è¿è¡Œæ¬¡æ•°

process = psutil.Process(os.getpid())

# ===== æµ‹è¯•æ•°æ®é…ç½® =====
TEST_DATASETS = [
    {
        "name": "small_csv",
        "local_path": "dacp://10.0.89.38:3101/GFS/small_100rows.csv",
        "local_real_path": "/data/faird/test-data/small_100rows.csv",
        "ftp_path": "remote/path/small_100rows.csv",
        "description": "å°æ–‡ä»¶ - 100è¡Œæ•°æ®",
        "expected_rows": 100
    },
    {
        "name": "medium_csv",
        "local_path": "dacp://60.245.194.25:50201/GFS/medium_10k_rows.csv",
        "local_real_path": "/data/faird/test-data/medium_10k_rows.csv",
        "ftp_path": "remote/path/medium_10k_rows.csv",
        "description": "ä¸­ç­‰æ–‡ä»¶ - 10Kè¡Œæ•°æ®",
        "expected_rows": 10000
    },
    {
        "name": "large_csv",
        "local_path": "dacp://60.245.194.25:50201/GFS/large_100k_rows.csv",
        "local_real_path": "/data/faird/test-data/large_100k_rows.csv",
        "ftp_path": "remote/path/large_100k_rows.csv",
        "description": "å¤§æ–‡ä»¶ - 100Kè¡Œæ•°æ®",
        "expected_rows": 100000
    },
    {
        "name": "xlarge_csv",
        "local_path": "dacp://60.245.194.25:50201/GFS/xlarge_1m_rows.csv",
        "local_real_path": "/data/faird/test-data/xlarge_1m_rows.csv",
        "ftp_path": "remote/path/xlarge_1m_rows.csv",
        "description": "è¶…å¤§æ–‡ä»¶ - 1Mè¡Œæ•°æ®",
        "expected_rows": 1000000
    },
    {
        "name": "xxlarge_csv",
        "local_path": "dacp://60.245.194.25:50201/GFS/xxlarge_10m_rows.csv",
        "local_real_path": "/data/faird/test-data/xxlarge_10m_rows.csv",
        "ftp_path": "remote/path/xxlarge_10m_rows.csv",
        "description": "è¶…å¤§æ–‡ä»¶ - 10Mè¡Œæ•°æ®",
        "expected_rows": 10000000
    }
]


# ===== æ•°æ®ç”Ÿæˆå‡½æ•° =====
def generate_test_csv_files():
    """ç”Ÿæˆä¸åŒå¤§å°çš„æµ‹è¯•CSVæ–‡ä»¶"""
    logger.info("å¼€å§‹ç”Ÿæˆæµ‹è¯•CSVæ–‡ä»¶...")

    # åŸºç¡€åˆ—ç»“æ„
    base_columns = [
        'æ²Ÿé“ç¼–å·', 'æ²Ÿé“åç§°', 'æ‰€å±åŒºå¿', 'é•¿åº¦_km', 'æ·±åº¦_m',
        'å®½åº¦_m', 'å¡åº¦_percent', 'åœŸå£¤ç±»å‹', 'æ¤è¢«è¦†ç›–ç‡', 'å¹´é™é›¨é‡_mm',
        'ç»åº¦', 'çº¬åº¦', 'æµ·æ‹”_m', 'å»ºè®¾å¹´ä»½', 'ç»´æŠ¤çŠ¶æ€'
    ]

    for dataset in TEST_DATASETS:
        file_path = dataset["local_path"]
        rows = dataset["expected_rows"]

        # åˆ›å»ºç›®å½•
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        if os.path.exists(file_path):
            logger.info(f"æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ç”Ÿæˆ: {file_path}")
            continue

        logger.info(f"ç”Ÿæˆ {dataset['description']} -> {file_path}")

        # ç”Ÿæˆéšæœºæ•°æ®
        data = {
            'æ²Ÿé“ç¼–å·': [f"GD{i:06d}" for i in range(1, rows + 1)],
            'æ²Ÿé“åç§°': [f"æ²Ÿé“_{i}" for i in range(1, rows + 1)],
            'æ‰€å±åŒºå¿': np.random.choice(['æ¦†é˜³åŒº', 'æ¨ªå±±åŒº', 'é–è¾¹å¿', 'å®šè¾¹å¿'], rows),
            'é•¿åº¦_km': np.round(np.random.uniform(0.5, 50.0, rows), 2),
            'æ·±åº¦_m': np.round(np.random.uniform(1.0, 15.0, rows), 2),
            'å®½åº¦_m': np.round(np.random.uniform(2.0, 25.0, rows), 2),
            'å¡åº¦_percent': np.round(np.random.uniform(1.0, 30.0, rows), 1),
            'åœŸå£¤ç±»å‹': np.random.choice(['é»„åœŸ', 'æ²™åœŸ', 'é»åœŸ', 'æ··åˆåœŸ'], rows),
            'æ¤è¢«è¦†ç›–ç‡': np.round(np.random.uniform(10.0, 90.0, rows), 1),
            'å¹´é™é›¨é‡_mm': np.random.randint(200, 800, rows),
            'ç»åº¦': np.round(np.random.uniform(107.0, 111.0, rows), 6),
            'çº¬åº¦': np.round(np.random.uniform(37.0, 40.0, rows), 6),
            'æµ·æ‹”_m': np.random.randint(800, 1800, rows),
            'å»ºè®¾å¹´ä»½': np.random.randint(1990, 2023, rows),
            'ç»´æŠ¤çŠ¶æ€': np.random.choice(['è‰¯å¥½', 'ä¸€èˆ¬', 'éœ€ç»´ä¿®', 'å·²æŸå'], rows)
        }

        df = pd.DataFrame(data)
        df.to_csv(file_path, index=False, encoding='utf-8')

        file_size_mb = os.path.getsize(file_path) / (1024 ** 2)
        logger.info(f"ç”Ÿæˆå®Œæˆ: {rows}è¡Œ, {file_size_mb:.2f}MB")


# ===== å·¥å…·å‡½æ•° =====
def get_mem_usage():
    return process.memory_info().rss / (1024 ** 2)


def get_file_size_mb(filepath):
    """è·å–æ–‡ä»¶å¤§å°(MB)"""
    if os.path.exists(filepath):
        return os.path.getsize(filepath) / (1024 ** 2)
    return 0


def flight_test(dataset):
    """Arrow Flightæµ‹è¯•"""
    logger.info(f"å¼€å§‹ Arrow Flight æµ‹è¯•: {dataset['description']}")
    mem_before = get_mem_usage()

    try:
        start_connect = time.time()
        conn = DacpClient.connect(SERVER_URL)
        connect_time = time.time() - start_connect

        start_open = time.time()
        df = conn.open(dataset['local_path'])
        df.collect()
        read_time = time.time() - start_open
        total_time = time.time() - start_connect

        mem_after = get_mem_usage()
        file_size_mb = get_file_size_mb(dataset['local_real_path'])

        result = {
            "dataset_name": dataset['name'],
            "dataset_desc": dataset['description'],
            "type": "flight",
            "connect_time": connect_time,
            "read_time": read_time,
            "total_time": total_time,
            "rows": df.num_rows,
            "cols": len(df.schema),
            "file_size_mb": file_size_mb,
            "throughput_mbps": file_size_mb / total_time if total_time > 0 else 0,
            "rows_per_sec": df.num_rows / total_time if total_time > 0 else 0,
            "memory_change_mb": mem_after - mem_before,
            "success": True
        }

        #conn.close()

    except Exception as e:
        logger.error(f"Flightæµ‹è¯•å¤±è´¥: {e}")
        result = {
            "dataset_name": dataset['name'],
            "dataset_desc": dataset['description'],
            "type": "flight",
            "error": str(e),
            "success": False
        }

    logger.info(f"Flight æµ‹è¯•å®Œæˆ: {dataset['name']}")
    return result


def ftp_test(dataset):
    """FTPæµ‹è¯•"""
    logger.info(f"å¼€å§‹ FTP æµ‹è¯•: {dataset['description']}")
    mem_before = get_mem_usage()
    local_temp_file = f"temp_{dataset['name']}.csv"

    try:
        start_connect = time.time()
        ftp = ftplib.FTP(FTP_HOST)
        ftp.login(user=FTP_USER, passwd=FTP_PASS)
        connect_time = time.time() - start_connect

        start_download = time.time()
        with open(local_temp_file, 'wb') as f:
            ftp.retrbinary(f"RETR {dataset['ftp_path']}", f.write)
        download_time = time.time() - start_download

        start_parse = time.time()
        df = pd.read_csv(local_temp_file)
        parse_time = time.time() - start_parse

        total_time = time.time() - start_connect
        mem_after = get_mem_usage()
        file_size_mb = get_file_size_mb(local_temp_file)

        result = {
            "dataset_name": dataset['name'],
            "dataset_desc": dataset['description'],
            "type": "ftp",
            "connect_time": connect_time,
            "download_time": download_time,
            "parse_time": parse_time,
            "total_time": total_time,
            "rows": len(df),
            "cols": len(df.columns),
            "file_size_mb": file_size_mb,
            "throughput_mbps": file_size_mb / total_time if total_time > 0 else 0,
            "rows_per_sec": len(df) / total_time if total_time > 0 else 0,
            "memory_change_mb": mem_after - mem_before,
            "success": True
        }

        ftp.quit()

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(local_temp_file):
            os.remove(local_temp_file)

    except Exception as e:
        logger.error(f"FTPæµ‹è¯•å¤±è´¥: {e}")
        result = {
            "dataset_name": dataset['name'],
            "dataset_desc": dataset['description'],
            "type": "ftp",
            "error": str(e),
            "success": False
        }

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(local_temp_file):
            os.remove(local_temp_file)

    logger.info(f"FTP æµ‹è¯•å®Œæˆ: {dataset['name']}")
    return result


# ===== æ‰¹é‡æµ‹è¯•å‡½æ•° =====
def run_single_protocol_tests(test_func, protocol_name, datasets, repeat_times=RUN_TIMES):
    """è¿è¡Œå•ä¸ªåè®®çš„æ‰€æœ‰æ•°æ®é›†æµ‹è¯•"""
    all_results = []

    for dataset in datasets:
        logger.info(f"\n--- {protocol_name} æµ‹è¯•æ•°æ®é›†: {dataset['description']} ---")
        dataset_results = []

        for i in range(repeat_times):
            logger.info(f"{protocol_name} {dataset['name']} ç¬¬ {i + 1}/{repeat_times} æ¬¡æµ‹è¯•")
            result = test_func(dataset)
            if result.get('success', False):
                dataset_results.append(result)
            time.sleep(1)  # é¿å…è¿æ¥è¿‡äºé¢‘ç¹

        all_results.extend(dataset_results)

        # è¾“å‡ºå½“å‰æ•°æ®é›†çš„ç»Ÿè®¡
        if dataset_results:
            avg_connect = sum(r["connect_time"] for r in dataset_results) / len(dataset_results)
            avg_total = sum(r["total_time"] for r in dataset_results) / len(dataset_results)
            avg_throughput = sum(r["throughput_mbps"] for r in dataset_results) / len(dataset_results)
            memory_change_mb = sum(r["memory_change_mb"] for r in dataset_results) / len(dataset_results)
            logger.info(
                f"{dataset['name']} å¹³å‡è¿æ¥æ—¶é—´: {avg_connect:.3f}s, æ€»æ—¶é—´: {avg_total:.3f}s, ååé‡: {avg_throughput:.2f}MB/s, å ç”¨å†…å­˜å˜åŒ–: {memory_change_mb:.2f}MB")

    return all_results


def run_concurrent_tests(test_func, protocol_name, dataset, concurrency=CONCURRENCY, repeat_times=1):
    """å¹¶å‘æµ‹è¯•"""
    all_results = []

    def worker():
        for _ in range(repeat_times):
            result = test_func(dataset)
            if result.get('success', False):
                all_results.append(result)

    logger.info(f"\n--- {protocol_name} å¹¶å‘æµ‹è¯• ({concurrency}çº¿ç¨‹): {dataset['description']} ---")

    threads = []
    start_time = time.time()

    for _ in range(concurrency):
        t = threading.Thread(target=worker)
        t.start()
        threads.append(t)

    for t in threads:
        t.join()

    concurrent_total_time = time.time() - start_time

    logger.info(f"{protocol_name} å¹¶å‘æµ‹è¯•å®Œæˆ: {len(all_results)}ä¸ªæˆåŠŸç»“æœ, æ€»è€—æ—¶: {concurrent_total_time:.2f}s")
    return all_results, concurrent_total_time


# ===== ç»“æœåˆ†æå‡½æ•° =====
def analyze_results(results, protocol_name):
    """åˆ†ææµ‹è¯•ç»“æœ"""
    if not results:
        logger.warning(f"{protocol_name} æ²¡æœ‰æœ‰æ•ˆç»“æœ")
        return {}

    # æŒ‰æ•°æ®é›†åˆ†ç»„ç»Ÿè®¡
    dataset_stats = {}
    for result in results:
        dataset_name = result['dataset_name']
        if dataset_name not in dataset_stats:
            dataset_stats[dataset_name] = []
        dataset_stats[dataset_name].append(result)

    analysis = {
        "protocol": protocol_name,
        "total_tests": len(results),
        "datasets": {}
    }

    for dataset_name, dataset_results in dataset_stats.items():
        if not dataset_results:
            continue

        stats = {
            "count": len(dataset_results),
            "avg_connect_time": sum(r["connect_time"] for r in dataset_results) / len(dataset_results),
            "avg_total_time": sum(r["total_time"] for r in dataset_results) / len(dataset_results),
            "avg_throughput_mbps": sum(r["throughput_mbps"] for r in dataset_results) / len(dataset_results),
            "avg_rows_per_sec": sum(r["rows_per_sec"] for r in dataset_results) / len(dataset_results),
            "avg_memory_change_mb": sum(r["memory_change_mb"] for r in dataset_results) / len(dataset_results),
            "avg_file_size_mb": sum(r["file_size_mb"] for r in dataset_results) / len(dataset_results),
            "min_total_time": min(r["total_time"] for r in dataset_results),
            "max_total_time": max(r["total_time"] for r in dataset_results)
        }

        analysis["datasets"][dataset_name] = stats

    return analysis


def print_comparison_report(flight_analysis, ftp_analysis):
    """æ‰“å°å¯¹æ¯”æŠ¥å‘Š"""
    logger.info("\n" + "=" * 80)
    logger.info("                        FAIRD vs FTP æ€§èƒ½å¯¹æ¯”æŠ¥å‘Š")
    logger.info("=" * 80)

    print(f"\n{'æ•°æ®é›†':<15} {'åè®®':<8} {'è¿æ¥æ—¶é—´(s)':<12} {'æ€»æ—¶é—´(s)':<10} {'ååé‡(MB/s)':<12} {'è¡Œ/ç§’':<10}")
    print("-" * 80)

    for dataset_name in flight_analysis.get("datasets", {}):
        flight_stats = flight_analysis["datasets"].get(dataset_name, {})
        ftp_stats = ftp_analysis["datasets"].get(dataset_name, {})

        if flight_stats:
            print(f"{dataset_name:<15} {'FAIRD':<8} {flight_stats['avg_connect_time']:<12.3f} "
                  f"{flight_stats['avg_total_time']:<10.3f} {flight_stats['avg_throughput_mbps']:<12.2f} "
                  f"{flight_stats['avg_rows_per_sec']:<10.0f}")

        if ftp_stats:
            print(f"{'':<15} {'FTP':<8} {ftp_stats['avg_connect_time']:<12.3f} "
                  f"{ftp_stats['avg_total_time']:<10.3f} {ftp_stats['avg_throughput_mbps']:<12.2f} "
                  f"{ftp_stats['avg_rows_per_sec']:<10.0f}")

        # è®¡ç®—æ€§èƒ½æ¯”è¾ƒ
        if flight_stats and ftp_stats:
            connect_ratio = ftp_stats['avg_connect_time'] / flight_stats['avg_connect_time']
            total_ratio = ftp_stats['avg_total_time'] / flight_stats['avg_total_time']
            throughput_ratio = flight_stats['avg_throughput_mbps'] / ftp_stats['avg_throughput_mbps']

            print(f"{'æ¯”è¾ƒ':<15} {'FAIRD':<8} {f'å¿«{connect_ratio:.1f}x':<12} "
                  f"{f'å¿«{total_ratio:.1f}x':<10} {f'å¿«{throughput_ratio:.1f}x':<12} {'':<10}")

        print("-" * 80)


# ===== ä¸»ç¨‹åº =====
if __name__ == "__main__":
    logger.info("====== FAIRD vs FTP æ€§èƒ½å¯¹æ¯”æµ‹è¯• ======")

    # 1. ç”Ÿæˆæµ‹è¯•æ•°æ®
    # generate_test_csv_files()

    # 2. å•çº¿ç¨‹æ€§èƒ½æµ‹è¯•
    logger.info("\nğŸš€ å¼€å§‹å•çº¿ç¨‹æ€§èƒ½æµ‹è¯•...")

    flight_results = run_single_protocol_tests(flight_test, "FAIRD", TEST_DATASETS, RUN_TIMES)
    ftp_results = run_single_protocol_tests(ftp_test, "FTP", TEST_DATASETS, RUN_TIMES)

    # 4. ç»“æœåˆ†æ
    logger.info("\nğŸ“Š åˆ†ææµ‹è¯•ç»“æœ...")

    flight_analysis = analyze_results(flight_results, "FAIRD")
    ftp_analysis = analyze_results(ftp_results, "FTP")

    # 5. è¾“å‡ºæŠ¥å‘Š
    print_comparison_report(flight_analysis, ftp_analysis)
    sys.exit(0)



    # 3. å¹¶å‘æ€§èƒ½æµ‹è¯•ï¼ˆé€‰æ‹©ä¸­ç­‰å¤§å°æ•°æ®é›†ï¼‰
    logger.info("\nğŸš€ å¼€å§‹å¹¶å‘æ€§èƒ½æµ‹è¯•...")

    medium_dataset = next((d for d in TEST_DATASETS if d['name'] == 'medium_csv'), TEST_DATASETS[1])

    flight_concurrent_results, flight_concurrent_time = run_concurrent_tests(
        flight_test, "FAIRD", medium_dataset, CONCURRENCY, 1)

    ftp_concurrent_results, ftp_concurrent_time = run_concurrent_tests(
        ftp_test, "FTP", medium_dataset, CONCURRENCY, 1)

    # 6. å¹¶å‘æµ‹è¯•ç»“æœ
    logger.info(f"\nğŸ”„ å¹¶å‘æµ‹è¯•ç»“æœ ({CONCURRENCY}çº¿ç¨‹):")
    logger.info(f"FAIRD: {len(flight_concurrent_results)}ä¸ªæˆåŠŸ, æ€»è€—æ—¶: {flight_concurrent_time:.2f}s")
    logger.info(f"FTP:   {len(ftp_concurrent_results)}ä¸ªæˆåŠŸ, æ€»è€—æ—¶: {ftp_concurrent_time:.2f}s")

    if flight_concurrent_results and ftp_concurrent_results:
        flight_avg_concurrent = sum(r["total_time"] for r in flight_concurrent_results) / len(flight_concurrent_results)
        ftp_avg_concurrent = sum(r["total_time"] for r in ftp_concurrent_results) / len(ftp_concurrent_results)
        logger.info(f"å¹¶å‘å¹³å‡å“åº”æ—¶é—´ - FAIRD: {flight_avg_concurrent:.3f}s, FTP: {ftp_avg_concurrent:.3f}s")

    logger.info("\nâœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")